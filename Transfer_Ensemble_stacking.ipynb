{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "573f0227",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Train CIFAR10 with PyTorch.'''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torchsummary\n",
    "\n",
    "#from models import *\n",
    "#from utils import progress_bar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2126278",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "parser = argparse.ArgumentParser(description='PyTorch CIFAR10 Training')\n",
    "parser.add_argument('--lr', default=0.001, type=float, help='learning rate')\n",
    "parser.add_argument('--resume', '-r', action='store_true',\n",
    "                    help='resume from checkpoint')\n",
    "args = parser.parse_args(args=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29c63254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "epoch_num = 25#300\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "best_acc = 0  # best test accuracy\n",
    "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33dbbe71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing data..\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Data\n",
    "print('==> Preparing data..')\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.ColorJitter(0.1,0.1,0.1,0.1),\n",
    "    #전이학습\n",
    "    transforms.Resize(256),\n",
    "    transforms.RandomCrop(224, padding=0),\n",
    "    #transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    \n",
    "    #과적합 방지 효과가 있다고 함\n",
    "    transforms.RandomRotation(3), \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize(224),#전이학습\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', \n",
    "    train=True,\n",
    "    download=True, \n",
    "    transform=transform_train)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset,\n",
    "    batch_size=32,\n",
    "    #batch_size=32,\n",
    "    shuffle=True,\n",
    "    num_workers=4)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(\n",
    "    root='./data',\n",
    "    train=False, \n",
    "    download=True,\n",
    "    transform=transform_test)\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset,\n",
    "    batch_size=32, \n",
    "    #batch_size=32, \n",
    "    shuffle=False, \n",
    "    num_workers=4)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a088f2ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Building model..\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "print('==> Building model..')\n",
    "# net = VGG('VGG19')\n",
    "\n",
    "# net = ResNet50()\n",
    "# 전이학습\n",
    "#net = torchvision.models.resnet50(pretrained=True)\n",
    "net = torchvision.models.resnet50(pretrained=True)\n",
    "num_fltrs = net.fc.in_features\n",
    "\n",
    "net = net.to(device)\n",
    "if device == 'cuda':\n",
    "    net = torch.nn.DataParallel(net)\n",
    "    cudnn.benchmark = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9da25627",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): ResNet(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (4): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (5): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5a266b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.resume:\n",
    "    # Load checkpoint.\n",
    "    print('==> Resuming from checkpoint..')\n",
    "    assert os.path.isdir('checkpoint'), 'Error: no checkpoint directory found!'\n",
    "    checkpoint = torch.load('./checkpoint/ckpt.pth')\n",
    "    net.load_state_dict(checkpoint['net'])\n",
    "    best_acc = checkpoint['acc']\n",
    "    start_epoch = checkpoint['epoch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af13de7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNewNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyNewNet, self).__init__()\n",
    "        self.resnet50 = torchvision.models.resnet50(pretrained=True)\n",
    "        self.linear1 = nn.Linear(512*4, 512)\n",
    "        self.bn1 = nn.BatchNorm1d(512)\n",
    "        self.drop1 = nn.Dropout(0.3)\n",
    "        self.linear2 = nn.Linear(512, 10)\n",
    "\n",
    "    # Forward Pass 정의 부분\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.resnet50(x)\n",
    "        #x = self.linear1(x)\n",
    "        x = F.gelu(self.drop1(self.bn1(self.linear1(x))))\n",
    "    \n",
    "        return F.softmax(self.linear2(x), dim=1)\n",
    "    \n",
    "\n",
    "class MyEnsemble(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(MyEnsemble, self).__init__()\n",
    "        self.resnet50 = torchvision.models.resnet50(pretrained=True)\n",
    "        self.mobilenet_v2 = torchvision.models.mobilenet_v2(pretrained=True)\n",
    "        #self.modelC = modelC\n",
    "\n",
    "        self.fc1 = nn.Linear(512*4+1280, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out1 = self.resnet50(x)\n",
    "        out2 = self.mobilenet_v2(x)\n",
    "        #out3 = self.modelC(x)\n",
    "\n",
    "        out = torch.cat((out1 + out2),1)# + out3\n",
    "\n",
    "        x = self.fc1(out)\n",
    "        return F.softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de9b1dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#my_model = MyNewNet()\n",
    "my_model = MyEnsemble()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f6d54093",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyEnsemble(\n",
       "  (resnet50): ResNet(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (4): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (5): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
       "  )\n",
       "  (mobilenet_v2): MobileNetV2(\n",
       "    (features): Sequential(\n",
       "      (0): ConvBNActivation(\n",
       "        (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6(inplace=True)\n",
       "      )\n",
       "      (1): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): ConvBNActivation(\n",
       "            (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (2): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): ConvBNActivation(\n",
       "            (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): ConvBNActivation(\n",
       "            (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
       "            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (3): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): ConvBNActivation(\n",
       "            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): ConvBNActivation(\n",
       "            (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
       "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (4): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): ConvBNActivation(\n",
       "            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): ConvBNActivation(\n",
       "            (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)\n",
       "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (5): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): ConvBNActivation(\n",
       "            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): ConvBNActivation(\n",
       "            (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (6): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): ConvBNActivation(\n",
       "            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): ConvBNActivation(\n",
       "            (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (7): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): ConvBNActivation(\n",
       "            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): ConvBNActivation(\n",
       "            (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n",
       "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (8): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): ConvBNActivation(\n",
       "            (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): ConvBNActivation(\n",
       "            (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (9): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): ConvBNActivation(\n",
       "            (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): ConvBNActivation(\n",
       "            (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (10): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): ConvBNActivation(\n",
       "            (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): ConvBNActivation(\n",
       "            (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (11): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): ConvBNActivation(\n",
       "            (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): ConvBNActivation(\n",
       "            (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (12): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): ConvBNActivation(\n",
       "            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): ConvBNActivation(\n",
       "            (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
       "            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (13): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): ConvBNActivation(\n",
       "            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): ConvBNActivation(\n",
       "            (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
       "            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (14): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): ConvBNActivation(\n",
       "            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): ConvBNActivation(\n",
       "            (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)\n",
       "            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (15): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): ConvBNActivation(\n",
       "            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): ConvBNActivation(\n",
       "            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (16): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): ConvBNActivation(\n",
       "            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): ConvBNActivation(\n",
       "            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (17): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): ConvBNActivation(\n",
       "            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): ConvBNActivation(\n",
       "            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (18): ConvBNActivation(\n",
       "        (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (classifier): Sequential(\n",
       "      (0): Dropout(p=0.2, inplace=False)\n",
       "      (1): Linear(in_features=1280, out_features=1000, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (fc1): Linear(in_features=3328, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "279d1b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in my_model.parameters():\n",
    "  param.requires_grad = True\n",
    "#for param in my_model.linear1.parameters():\n",
    "#  param.requires_grad = True\n",
    "#for param in my_model.linear2.parameters():\n",
    "#  param.requires_grad = True\n",
    "for param in my_model.fc1.parameters():\n",
    "  param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f3a3eaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-pytorch from version 0.0.5.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  ---------\n",
      "adabelief-pytorch=0.0.5  1e-08  False              False\n",
      ">=0.1.0 (Current 0.2.0)  1e-16  True               True\n",
      "\u001b[34mSGD better than Adam (e.g. CNN for Image Classification)    Adam better than SGD (e.g. Transformer, GAN)\n",
      "----------------------------------------------------------  ----------------------------------------------\n",
      "Recommended eps = 1e-8                                      Recommended eps = 1e-16\n",
      "\u001b[34mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[34mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[32mYou can disable the log message by setting \"print_change_log = False\", though it is recommended to keep as a reminder.\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from adabelief_pytorch import AdaBelief\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#optimizer = optim.SGD(net.parameters(), lr=args.lr, momentum=0.9, weight_decay=5e-4)\n",
    "#optimizer = optim.Adam(net.parameters(), lr=args.lr)\n",
    "optimizer=AdaBelief(net.parameters(), lr=0.0001, eps =1e-16, betas=(0.9, 0.999),\n",
    "                    weight_decay = 5e-4, weight_decouple=False, rectify=False,\n",
    "                    fixed_decay=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff5c40f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epoch_num, last_epoch=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aa85675b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "        if batch_idx % 50 == 0:\n",
    "            print('Epoch: {} | Batch_idx: {} |  Loss: ({:.4f}) | Acc: ({:.2f}%) ({}/{})'.format(epoch, batch_idx, train_loss / (batch_idx + 1), 100. * correct / total, correct, total))\n",
    "    print('Epoch: {} | Batch_idx: {} |  Loss: ({:.4f}) | Acc: ({:.2f}%) ({}/{})'.format(epoch, batch_idx, train_loss / (batch_idx + 1), 100. * correct / total, correct, total))\n",
    "    return (100. * correct / total)\n",
    "\n",
    "def test(epoch):\n",
    "    global best_acc\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        print('# TEST : Loss: ({:.4f}) | Acc: ({:.2f}%) ({}/{})'.format(test_loss / (batch_idx + 1), 100. * correct / total, correct, total))\n",
    "\n",
    "    # Save checkpoint.\n",
    "    acc = 100.*correct/total\n",
    "    if acc > best_acc:\n",
    "        #  print('Saving..')\n",
    "        state = {\n",
    "            'net': net.state_dict(),\n",
    "            'acc': acc,\n",
    "            'epoch': epoch,\n",
    "        }\n",
    "        if not os.path.isdir('checkpoint'):\n",
    "            os.mkdir('checkpoint')\n",
    "        #torch.save(state, './checkpoint/ckpt.pth')\n",
    "        best_acc = acc\n",
    "    return best_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8a89ff68",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_error = []\n",
    "test_error = []\n",
    "\n",
    "lerningrate_temp = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e9c079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CPSC_M\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ..\\c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Batch_idx: 0 |  Loss: (9.0679) | Acc: (0.00%) (0/32)\n",
      "Epoch: 0 | Batch_idx: 50 |  Loss: (2.6272) | Acc: (48.16%) (786/1632)\n",
      "Epoch: 0 | Batch_idx: 100 |  Loss: (1.7595) | Acc: (59.84%) (1934/3232)\n",
      "Epoch: 0 | Batch_idx: 150 |  Loss: (1.4002) | Acc: (65.75%) (3177/4832)\n",
      "Epoch: 0 | Batch_idx: 200 |  Loss: (1.1877) | Acc: (69.64%) (4479/6432)\n",
      "Epoch: 0 | Batch_idx: 250 |  Loss: (1.0532) | Acc: (72.39%) (5814/8032)\n",
      "Epoch: 0 | Batch_idx: 300 |  Loss: (0.9578) | Acc: (74.19%) (7146/9632)\n",
      "Epoch: 0 | Batch_idx: 350 |  Loss: (0.8901) | Acc: (75.64%) (8496/11232)\n",
      "Epoch: 0 | Batch_idx: 400 |  Loss: (0.8336) | Acc: (76.87%) (9864/12832)\n",
      "Epoch: 0 | Batch_idx: 450 |  Loss: (0.7885) | Acc: (77.81%) (11229/14432)\n",
      "Epoch: 0 | Batch_idx: 500 |  Loss: (0.7556) | Acc: (78.53%) (12590/16032)\n",
      "Epoch: 0 | Batch_idx: 550 |  Loss: (0.7250) | Acc: (79.22%) (13968/17632)\n",
      "Epoch: 0 | Batch_idx: 600 |  Loss: (0.6996) | Acc: (79.88%) (15363/19232)\n",
      "Epoch: 0 | Batch_idx: 650 |  Loss: (0.6756) | Acc: (80.43%) (16756/20832)\n",
      "Epoch: 0 | Batch_idx: 700 |  Loss: (0.6547) | Acc: (80.79%) (18122/22432)\n",
      "Epoch: 0 | Batch_idx: 750 |  Loss: (0.6374) | Acc: (81.17%) (19507/24032)\n",
      "Epoch: 0 | Batch_idx: 800 |  Loss: (0.6234) | Acc: (81.50%) (20891/25632)\n",
      "Epoch: 0 | Batch_idx: 850 |  Loss: (0.6077) | Acc: (81.87%) (22294/27232)\n",
      "Epoch: 0 | Batch_idx: 900 |  Loss: (0.5944) | Acc: (82.19%) (23697/28832)\n",
      "Epoch: 0 | Batch_idx: 950 |  Loss: (0.5800) | Acc: (82.56%) (25125/30432)\n",
      "Epoch: 0 | Batch_idx: 1000 |  Loss: (0.5700) | Acc: (82.81%) (26526/32032)\n",
      "Epoch: 0 | Batch_idx: 1050 |  Loss: (0.5593) | Acc: (83.08%) (27943/33632)\n",
      "Epoch: 0 | Batch_idx: 1100 |  Loss: (0.5492) | Acc: (83.34%) (29363/35232)\n",
      "Epoch: 0 | Batch_idx: 1150 |  Loss: (0.5401) | Acc: (83.60%) (30790/36832)\n",
      "Epoch: 0 | Batch_idx: 1200 |  Loss: (0.5328) | Acc: (83.75%) (32186/38432)\n",
      "Epoch: 0 | Batch_idx: 1250 |  Loss: (0.5261) | Acc: (83.90%) (33586/40032)\n",
      "Epoch: 0 | Batch_idx: 1300 |  Loss: (0.5190) | Acc: (84.08%) (35004/41632)\n",
      "Epoch: 0 | Batch_idx: 1350 |  Loss: (0.5116) | Acc: (84.27%) (36431/43232)\n",
      "Epoch: 0 | Batch_idx: 1400 |  Loss: (0.5040) | Acc: (84.48%) (37875/44832)\n",
      "Epoch: 0 | Batch_idx: 1450 |  Loss: (0.4991) | Acc: (84.59%) (39275/46432)\n",
      "Epoch: 0 | Batch_idx: 1500 |  Loss: (0.4939) | Acc: (84.71%) (40687/48032)\n",
      "Epoch: 0 | Batch_idx: 1550 |  Loss: (0.4879) | Acc: (84.88%) (42129/49632)\n",
      "Epoch: 0 | Batch_idx: 1562 |  Loss: (0.4869) | Acc: (84.91%) (42456/50000)\n",
      "# TEST : Loss: (0.2282) | Acc: (92.17%) (9217/10000)\n",
      "9.96057350657239e-05\n",
      "\n",
      "Epoch: 1\n",
      "Epoch: 1 | Batch_idx: 0 |  Loss: (0.1768) | Acc: (93.75%) (30/32)\n",
      "Epoch: 1 | Batch_idx: 50 |  Loss: (0.2410) | Acc: (91.97%) (1501/1632)\n",
      "Epoch: 1 | Batch_idx: 100 |  Loss: (0.2446) | Acc: (91.65%) (2962/3232)\n",
      "Epoch: 1 | Batch_idx: 150 |  Loss: (0.2520) | Acc: (91.43%) (4418/4832)\n",
      "Epoch: 1 | Batch_idx: 200 |  Loss: (0.2497) | Acc: (91.48%) (5884/6432)\n",
      "Epoch: 1 | Batch_idx: 250 |  Loss: (0.2470) | Acc: (91.62%) (7359/8032)\n",
      "Epoch: 1 | Batch_idx: 300 |  Loss: (0.2535) | Acc: (91.43%) (8807/9632)\n",
      "Epoch: 1 | Batch_idx: 350 |  Loss: (0.2581) | Acc: (91.35%) (10260/11232)\n",
      "Epoch: 1 | Batch_idx: 400 |  Loss: (0.2613) | Acc: (91.19%) (11702/12832)\n",
      "Epoch: 1 | Batch_idx: 450 |  Loss: (0.2658) | Acc: (90.99%) (13131/14432)\n",
      "Epoch: 1 | Batch_idx: 500 |  Loss: (0.2663) | Acc: (91.01%) (14590/16032)\n",
      "Epoch: 1 | Batch_idx: 550 |  Loss: (0.2668) | Acc: (91.07%) (16057/17632)\n",
      "Epoch: 1 | Batch_idx: 600 |  Loss: (0.2677) | Acc: (91.03%) (17507/19232)\n",
      "Epoch: 1 | Batch_idx: 650 |  Loss: (0.2667) | Acc: (91.05%) (18968/20832)\n",
      "Epoch: 1 | Batch_idx: 700 |  Loss: (0.2654) | Acc: (91.09%) (20434/22432)\n",
      "Epoch: 1 | Batch_idx: 750 |  Loss: (0.2666) | Acc: (91.07%) (21886/24032)\n",
      "Epoch: 1 | Batch_idx: 800 |  Loss: (0.2683) | Acc: (91.08%) (23346/25632)\n",
      "Epoch: 1 | Batch_idx: 850 |  Loss: (0.2700) | Acc: (91.03%) (24790/27232)\n",
      "Epoch: 1 | Batch_idx: 900 |  Loss: (0.2708) | Acc: (91.00%) (26236/28832)\n",
      "Epoch: 1 | Batch_idx: 950 |  Loss: (0.2706) | Acc: (91.00%) (27694/30432)\n",
      "Epoch: 1 | Batch_idx: 1000 |  Loss: (0.2718) | Acc: (90.95%) (29134/32032)\n",
      "Epoch: 1 | Batch_idx: 1050 |  Loss: (0.2716) | Acc: (90.93%) (30582/33632)\n",
      "Epoch: 1 | Batch_idx: 1100 |  Loss: (0.2724) | Acc: (90.94%) (32040/35232)\n",
      "Epoch: 1 | Batch_idx: 1150 |  Loss: (0.2728) | Acc: (90.94%) (33495/36832)\n",
      "Epoch: 1 | Batch_idx: 1200 |  Loss: (0.2716) | Acc: (90.99%) (34968/38432)\n",
      "Epoch: 1 | Batch_idx: 1250 |  Loss: (0.2726) | Acc: (90.96%) (36414/40032)\n",
      "Epoch: 1 | Batch_idx: 1300 |  Loss: (0.2724) | Acc: (90.96%) (37867/41632)\n",
      "Epoch: 1 | Batch_idx: 1350 |  Loss: (0.2716) | Acc: (91.00%) (39343/43232)\n",
      "Epoch: 1 | Batch_idx: 1400 |  Loss: (0.2709) | Acc: (91.00%) (40796/44832)\n",
      "Epoch: 1 | Batch_idx: 1450 |  Loss: (0.2714) | Acc: (91.00%) (42252/46432)\n",
      "Epoch: 1 | Batch_idx: 1500 |  Loss: (0.2715) | Acc: (91.00%) (43707/48032)\n",
      "Epoch: 1 | Batch_idx: 1550 |  Loss: (0.2710) | Acc: (90.99%) (45162/49632)\n",
      "Epoch: 1 | Batch_idx: 1562 |  Loss: (0.2716) | Acc: (90.97%) (45486/50000)\n",
      "# TEST : Loss: (0.2315) | Acc: (92.41%) (9241/10000)\n",
      "9.842915805643155e-05\n",
      "\n",
      "Epoch: 2\n",
      "Epoch: 2 | Batch_idx: 0 |  Loss: (0.4557) | Acc: (87.50%) (28/32)\n",
      "Epoch: 2 | Batch_idx: 50 |  Loss: (0.2515) | Acc: (92.03%) (1502/1632)\n",
      "Epoch: 2 | Batch_idx: 100 |  Loss: (0.2459) | Acc: (92.30%) (2983/3232)\n",
      "Epoch: 2 | Batch_idx: 150 |  Loss: (0.2390) | Acc: (92.12%) (4451/4832)\n",
      "Epoch: 2 | Batch_idx: 200 |  Loss: (0.2367) | Acc: (92.16%) (5928/6432)\n",
      "Epoch: 2 | Batch_idx: 250 |  Loss: (0.2329) | Acc: (92.27%) (7411/8032)\n",
      "Epoch: 2 | Batch_idx: 300 |  Loss: (0.2309) | Acc: (92.21%) (8882/9632)\n",
      "Epoch: 2 | Batch_idx: 350 |  Loss: (0.2323) | Acc: (92.18%) (10354/11232)\n",
      "Epoch: 2 | Batch_idx: 400 |  Loss: (0.2305) | Acc: (92.22%) (11834/12832)\n",
      "Epoch: 2 | Batch_idx: 450 |  Loss: (0.2319) | Acc: (92.11%) (13294/14432)\n",
      "Epoch: 2 | Batch_idx: 500 |  Loss: (0.2336) | Acc: (92.11%) (14767/16032)\n",
      "Epoch: 2 | Batch_idx: 550 |  Loss: (0.2343) | Acc: (92.10%) (16239/17632)\n",
      "Epoch: 2 | Batch_idx: 600 |  Loss: (0.2346) | Acc: (92.05%) (17703/19232)\n",
      "Epoch: 2 | Batch_idx: 650 |  Loss: (0.2348) | Acc: (92.05%) (19175/20832)\n",
      "Epoch: 2 | Batch_idx: 700 |  Loss: (0.2321) | Acc: (92.14%) (20669/22432)\n",
      "Epoch: 2 | Batch_idx: 750 |  Loss: (0.2334) | Acc: (92.13%) (22141/24032)\n",
      "Epoch: 2 | Batch_idx: 800 |  Loss: (0.2362) | Acc: (92.05%) (23593/25632)\n",
      "Epoch: 2 | Batch_idx: 850 |  Loss: (0.2340) | Acc: (92.16%) (25096/27232)\n",
      "Epoch: 2 | Batch_idx: 900 |  Loss: (0.2337) | Acc: (92.18%) (26576/28832)\n",
      "Epoch: 2 | Batch_idx: 950 |  Loss: (0.2340) | Acc: (92.20%) (28059/30432)\n",
      "Epoch: 2 | Batch_idx: 1000 |  Loss: (0.2346) | Acc: (92.16%) (29522/32032)\n",
      "Epoch: 2 | Batch_idx: 1050 |  Loss: (0.2358) | Acc: (92.09%) (30973/33632)\n",
      "Epoch: 2 | Batch_idx: 1100 |  Loss: (0.2365) | Acc: (92.10%) (32450/35232)\n",
      "Epoch: 2 | Batch_idx: 1150 |  Loss: (0.2359) | Acc: (92.14%) (33936/36832)\n",
      "Epoch: 2 | Batch_idx: 1200 |  Loss: (0.2346) | Acc: (92.17%) (35422/38432)\n",
      "Epoch: 2 | Batch_idx: 1250 |  Loss: (0.2328) | Acc: (92.24%) (36924/40032)\n",
      "Epoch: 2 | Batch_idx: 1300 |  Loss: (0.2339) | Acc: (92.20%) (38385/41632)\n",
      "Epoch: 2 | Batch_idx: 1350 |  Loss: (0.2343) | Acc: (92.19%) (39854/43232)\n",
      "Epoch: 2 | Batch_idx: 1400 |  Loss: (0.2340) | Acc: (92.21%) (41338/44832)\n",
      "Epoch: 2 | Batch_idx: 1450 |  Loss: (0.2346) | Acc: (92.16%) (42794/46432)\n",
      "Epoch: 2 | Batch_idx: 1500 |  Loss: (0.2340) | Acc: (92.19%) (44281/48032)\n",
      "Epoch: 2 | Batch_idx: 1550 |  Loss: (0.2345) | Acc: (92.18%) (45750/49632)\n",
      "Epoch: 2 | Batch_idx: 1562 |  Loss: (0.2348) | Acc: (92.16%) (46078/50000)\n",
      "# TEST : Loss: (0.1898) | Acc: (93.70%) (9370/10000)\n",
      "9.648882429441257e-05\n",
      "\n",
      "Epoch: 3\n",
      "Epoch: 3 | Batch_idx: 0 |  Loss: (0.3384) | Acc: (81.25%) (26/32)\n",
      "Epoch: 3 | Batch_idx: 50 |  Loss: (0.1962) | Acc: (93.50%) (1526/1632)\n",
      "Epoch: 3 | Batch_idx: 100 |  Loss: (0.1884) | Acc: (93.72%) (3029/3232)\n",
      "Epoch: 3 | Batch_idx: 150 |  Loss: (0.1839) | Acc: (93.89%) (4537/4832)\n",
      "Epoch: 3 | Batch_idx: 200 |  Loss: (0.1878) | Acc: (93.63%) (6022/6432)\n",
      "Epoch: 3 | Batch_idx: 250 |  Loss: (0.1816) | Acc: (93.76%) (7531/8032)\n",
      "Epoch: 3 | Batch_idx: 300 |  Loss: (0.1836) | Acc: (93.71%) (9026/9632)\n",
      "Epoch: 3 | Batch_idx: 350 |  Loss: (0.1879) | Acc: (93.55%) (10507/11232)\n",
      "Epoch: 3 | Batch_idx: 400 |  Loss: (0.1937) | Acc: (93.34%) (11977/12832)\n",
      "Epoch: 3 | Batch_idx: 450 |  Loss: (0.1952) | Acc: (93.34%) (13471/14432)\n",
      "Epoch: 3 | Batch_idx: 500 |  Loss: (0.1970) | Acc: (93.29%) (14957/16032)\n",
      "Epoch: 3 | Batch_idx: 550 |  Loss: (0.1985) | Acc: (93.22%) (16437/17632)\n",
      "Epoch: 3 | Batch_idx: 600 |  Loss: (0.1998) | Acc: (93.21%) (17926/19232)\n",
      "Epoch: 3 | Batch_idx: 650 |  Loss: (0.2016) | Acc: (93.08%) (19390/20832)\n",
      "Epoch: 3 | Batch_idx: 700 |  Loss: (0.2037) | Acc: (93.05%) (20873/22432)\n",
      "Epoch: 3 | Batch_idx: 750 |  Loss: (0.2047) | Acc: (93.02%) (22355/24032)\n",
      "Epoch: 3 | Batch_idx: 800 |  Loss: (0.2064) | Acc: (92.97%) (23831/25632)\n",
      "Epoch: 3 | Batch_idx: 850 |  Loss: (0.2060) | Acc: (92.97%) (25318/27232)\n",
      "Epoch: 3 | Batch_idx: 900 |  Loss: (0.2066) | Acc: (93.01%) (26816/28832)\n",
      "Epoch: 3 | Batch_idx: 950 |  Loss: (0.2087) | Acc: (92.92%) (28276/30432)\n",
      "Epoch: 3 | Batch_idx: 1000 |  Loss: (0.2084) | Acc: (92.94%) (29771/32032)\n",
      "Epoch: 3 | Batch_idx: 1050 |  Loss: (0.2098) | Acc: (92.92%) (31251/33632)\n",
      "Epoch: 3 | Batch_idx: 1100 |  Loss: (0.2110) | Acc: (92.90%) (32732/35232)\n",
      "Epoch: 3 | Batch_idx: 1150 |  Loss: (0.2103) | Acc: (92.92%) (34224/36832)\n",
      "Epoch: 3 | Batch_idx: 1200 |  Loss: (0.2104) | Acc: (92.92%) (35710/38432)\n",
      "Epoch: 3 | Batch_idx: 1250 |  Loss: (0.2107) | Acc: (92.88%) (37183/40032)\n",
      "Epoch: 3 | Batch_idx: 1300 |  Loss: (0.2109) | Acc: (92.88%) (38669/41632)\n",
      "Epoch: 3 | Batch_idx: 1350 |  Loss: (0.2104) | Acc: (92.90%) (40163/43232)\n",
      "Epoch: 3 | Batch_idx: 1400 |  Loss: (0.2098) | Acc: (92.93%) (41661/44832)\n",
      "Epoch: 3 | Batch_idx: 1450 |  Loss: (0.2101) | Acc: (92.93%) (43147/46432)\n",
      "Epoch: 3 | Batch_idx: 1500 |  Loss: (0.2121) | Acc: (92.88%) (44610/48032)\n",
      "Epoch: 3 | Batch_idx: 1550 |  Loss: (0.2120) | Acc: (92.87%) (46095/49632)\n",
      "Epoch: 3 | Batch_idx: 1562 |  Loss: (0.2118) | Acc: (92.88%) (46438/50000)\n",
      "# TEST : Loss: (0.2310) | Acc: (92.45%) (9245/10000)\n",
      "9.381533400219318e-05\n",
      "\n",
      "Epoch: 4\n",
      "Epoch: 4 | Batch_idx: 0 |  Loss: (0.1000) | Acc: (96.88%) (31/32)\n",
      "Epoch: 4 | Batch_idx: 50 |  Loss: (0.1843) | Acc: (93.93%) (1533/1632)\n",
      "Epoch: 4 | Batch_idx: 100 |  Loss: (0.1738) | Acc: (94.18%) (3044/3232)\n",
      "Epoch: 4 | Batch_idx: 150 |  Loss: (0.1773) | Acc: (94.12%) (4548/4832)\n",
      "Epoch: 4 | Batch_idx: 200 |  Loss: (0.1815) | Acc: (93.75%) (6030/6432)\n",
      "Epoch: 4 | Batch_idx: 250 |  Loss: (0.1845) | Acc: (93.56%) (7515/8032)\n",
      "Epoch: 4 | Batch_idx: 300 |  Loss: (0.1888) | Acc: (93.41%) (8997/9632)\n",
      "Epoch: 4 | Batch_idx: 350 |  Loss: (0.1888) | Acc: (93.34%) (10484/11232)\n",
      "Epoch: 4 | Batch_idx: 400 |  Loss: (0.1867) | Acc: (93.43%) (11989/12832)\n",
      "Epoch: 4 | Batch_idx: 450 |  Loss: (0.1845) | Acc: (93.53%) (13498/14432)\n",
      "Epoch: 4 | Batch_idx: 500 |  Loss: (0.1876) | Acc: (93.41%) (14976/16032)\n",
      "Epoch: 4 | Batch_idx: 550 |  Loss: (0.1904) | Acc: (93.39%) (16467/17632)\n",
      "Epoch: 4 | Batch_idx: 600 |  Loss: (0.1898) | Acc: (93.47%) (17976/19232)\n",
      "Epoch: 4 | Batch_idx: 650 |  Loss: (0.1883) | Acc: (93.51%) (19479/20832)\n",
      "Epoch: 4 | Batch_idx: 700 |  Loss: (0.1884) | Acc: (93.50%) (20973/22432)\n",
      "Epoch: 4 | Batch_idx: 750 |  Loss: (0.1894) | Acc: (93.49%) (22467/24032)\n",
      "Epoch: 4 | Batch_idx: 800 |  Loss: (0.1884) | Acc: (93.53%) (23974/25632)\n",
      "Epoch: 4 | Batch_idx: 850 |  Loss: (0.1898) | Acc: (93.50%) (25462/27232)\n",
      "Epoch: 4 | Batch_idx: 900 |  Loss: (0.1925) | Acc: (93.42%) (26934/28832)\n",
      "Epoch: 4 | Batch_idx: 950 |  Loss: (0.1936) | Acc: (93.39%) (28419/30432)\n",
      "Epoch: 4 | Batch_idx: 1000 |  Loss: (0.1939) | Acc: (93.40%) (29917/32032)\n",
      "Epoch: 4 | Batch_idx: 1050 |  Loss: (0.1944) | Acc: (93.40%) (31412/33632)\n",
      "Epoch: 4 | Batch_idx: 1100 |  Loss: (0.1950) | Acc: (93.38%) (32901/35232)\n",
      "Epoch: 4 | Batch_idx: 1150 |  Loss: (0.1957) | Acc: (93.35%) (34382/36832)\n",
      "Epoch: 4 | Batch_idx: 1200 |  Loss: (0.1953) | Acc: (93.38%) (35886/38432)\n",
      "Epoch: 4 | Batch_idx: 1250 |  Loss: (0.1949) | Acc: (93.37%) (37378/40032)\n",
      "Epoch: 4 | Batch_idx: 1300 |  Loss: (0.1947) | Acc: (93.37%) (38870/41632)\n",
      "Epoch: 4 | Batch_idx: 1350 |  Loss: (0.1934) | Acc: (93.39%) (40374/43232)\n",
      "Epoch: 4 | Batch_idx: 1400 |  Loss: (0.1933) | Acc: (93.41%) (41876/44832)\n",
      "Epoch: 4 | Batch_idx: 1450 |  Loss: (0.1935) | Acc: (93.39%) (43362/46432)\n",
      "Epoch: 4 | Batch_idx: 1500 |  Loss: (0.1947) | Acc: (93.36%) (44843/48032)\n",
      "Epoch: 4 | Batch_idx: 1550 |  Loss: (0.1951) | Acc: (93.34%) (46328/49632)\n",
      "Epoch: 4 | Batch_idx: 1562 |  Loss: (0.1951) | Acc: (93.34%) (46669/50000)\n",
      "# TEST : Loss: (0.1817) | Acc: (93.97%) (9397/10000)\n",
      "9.045084971874738e-05\n",
      "\n",
      "Epoch: 5\n",
      "Epoch: 5 | Batch_idx: 0 |  Loss: (0.1309) | Acc: (96.88%) (31/32)\n",
      "Epoch: 5 | Batch_idx: 50 |  Loss: (0.1537) | Acc: (95.28%) (1555/1632)\n",
      "Epoch: 5 | Batch_idx: 100 |  Loss: (0.1647) | Acc: (94.71%) (3061/3232)\n",
      "Epoch: 5 | Batch_idx: 150 |  Loss: (0.1530) | Acc: (95.16%) (4598/4832)\n",
      "Epoch: 5 | Batch_idx: 200 |  Loss: (0.1510) | Acc: (95.13%) (6119/6432)\n",
      "Epoch: 5 | Batch_idx: 250 |  Loss: (0.1492) | Acc: (95.22%) (7648/8032)\n",
      "Epoch: 5 | Batch_idx: 300 |  Loss: (0.1525) | Acc: (95.07%) (9157/9632)\n",
      "Epoch: 5 | Batch_idx: 350 |  Loss: (0.1558) | Acc: (94.89%) (10658/11232)\n",
      "Epoch: 5 | Batch_idx: 400 |  Loss: (0.1588) | Acc: (94.79%) (12163/12832)\n",
      "Epoch: 5 | Batch_idx: 450 |  Loss: (0.1589) | Acc: (94.80%) (13682/14432)\n",
      "Epoch: 5 | Batch_idx: 500 |  Loss: (0.1594) | Acc: (94.77%) (15194/16032)\n",
      "Epoch: 5 | Batch_idx: 550 |  Loss: (0.1608) | Acc: (94.73%) (16702/17632)\n",
      "Epoch: 5 | Batch_idx: 600 |  Loss: (0.1622) | Acc: (94.63%) (18200/19232)\n",
      "Epoch: 5 | Batch_idx: 650 |  Loss: (0.1631) | Acc: (94.58%) (19702/20832)\n",
      "Epoch: 5 | Batch_idx: 700 |  Loss: (0.1646) | Acc: (94.53%) (21205/22432)\n",
      "Epoch: 5 | Batch_idx: 750 |  Loss: (0.1656) | Acc: (94.47%) (22704/24032)\n",
      "Epoch: 5 | Batch_idx: 800 |  Loss: (0.1666) | Acc: (94.43%) (24204/25632)\n",
      "Epoch: 5 | Batch_idx: 850 |  Loss: (0.1698) | Acc: (94.29%) (25677/27232)\n",
      "Epoch: 5 | Batch_idx: 900 |  Loss: (0.1676) | Acc: (94.36%) (27207/28832)\n",
      "Epoch: 5 | Batch_idx: 950 |  Loss: (0.1695) | Acc: (94.33%) (28705/30432)\n",
      "Epoch: 5 | Batch_idx: 1000 |  Loss: (0.1707) | Acc: (94.27%) (30196/32032)\n",
      "Epoch: 5 | Batch_idx: 1050 |  Loss: (0.1703) | Acc: (94.28%) (31708/33632)\n",
      "Epoch: 5 | Batch_idx: 1100 |  Loss: (0.1696) | Acc: (94.31%) (33227/35232)\n",
      "Epoch: 5 | Batch_idx: 1150 |  Loss: (0.1706) | Acc: (94.29%) (34729/36832)\n",
      "Epoch: 5 | Batch_idx: 1200 |  Loss: (0.1706) | Acc: (94.30%) (36240/38432)\n",
      "Epoch: 5 | Batch_idx: 1250 |  Loss: (0.1708) | Acc: (94.28%) (37744/40032)\n",
      "Epoch: 5 | Batch_idx: 1300 |  Loss: (0.1715) | Acc: (94.25%) (39237/41632)\n",
      "Epoch: 5 | Batch_idx: 1350 |  Loss: (0.1718) | Acc: (94.21%) (40731/43232)\n",
      "Epoch: 5 | Batch_idx: 1400 |  Loss: (0.1727) | Acc: (94.20%) (42230/44832)\n",
      "Epoch: 5 | Batch_idx: 1450 |  Loss: (0.1737) | Acc: (94.17%) (43724/46432)\n",
      "Epoch: 5 | Batch_idx: 1500 |  Loss: (0.1740) | Acc: (94.15%) (45224/48032)\n",
      "Epoch: 5 | Batch_idx: 1550 |  Loss: (0.1745) | Acc: (94.13%) (46720/49632)\n",
      "Epoch: 5 | Batch_idx: 1562 |  Loss: (0.1745) | Acc: (94.13%) (47066/50000)\n",
      "# TEST : Loss: (0.1719) | Acc: (94.29%) (9429/10000)\n",
      "8.644843137107059e-05\n",
      "\n",
      "Epoch: 6\n",
      "Epoch: 6 | Batch_idx: 0 |  Loss: (0.2729) | Acc: (90.62%) (29/32)\n",
      "Epoch: 6 | Batch_idx: 50 |  Loss: (0.1201) | Acc: (96.26%) (1571/1632)\n",
      "Epoch: 6 | Batch_idx: 100 |  Loss: (0.1341) | Acc: (95.67%) (3092/3232)\n",
      "Epoch: 6 | Batch_idx: 150 |  Loss: (0.1376) | Acc: (95.53%) (4616/4832)\n",
      "Epoch: 6 | Batch_idx: 200 |  Loss: (0.1397) | Acc: (95.40%) (6136/6432)\n",
      "Epoch: 6 | Batch_idx: 250 |  Loss: (0.1446) | Acc: (95.28%) (7653/8032)\n",
      "Epoch: 6 | Batch_idx: 300 |  Loss: (0.1462) | Acc: (95.26%) (9175/9632)\n",
      "Epoch: 6 | Batch_idx: 350 |  Loss: (0.1448) | Acc: (95.33%) (10708/11232)\n",
      "Epoch: 6 | Batch_idx: 400 |  Loss: (0.1475) | Acc: (95.15%) (12210/12832)\n",
      "Epoch: 6 | Batch_idx: 450 |  Loss: (0.1485) | Acc: (95.10%) (13725/14432)\n",
      "Epoch: 6 | Batch_idx: 500 |  Loss: (0.1472) | Acc: (95.12%) (15250/16032)\n",
      "Epoch: 6 | Batch_idx: 550 |  Loss: (0.1463) | Acc: (95.16%) (16779/17632)\n",
      "Epoch: 6 | Batch_idx: 600 |  Loss: (0.1499) | Acc: (95.07%) (18283/19232)\n",
      "Epoch: 6 | Batch_idx: 650 |  Loss: (0.1503) | Acc: (95.08%) (19808/20832)\n",
      "Epoch: 6 | Batch_idx: 700 |  Loss: (0.1511) | Acc: (95.06%) (21323/22432)\n",
      "Epoch: 6 | Batch_idx: 750 |  Loss: (0.1523) | Acc: (95.00%) (22831/24032)\n",
      "Epoch: 6 | Batch_idx: 800 |  Loss: (0.1522) | Acc: (94.99%) (24347/25632)\n",
      "Epoch: 6 | Batch_idx: 850 |  Loss: (0.1524) | Acc: (94.98%) (25866/27232)\n",
      "Epoch: 6 | Batch_idx: 900 |  Loss: (0.1524) | Acc: (94.97%) (27381/28832)\n",
      "Epoch: 6 | Batch_idx: 950 |  Loss: (0.1536) | Acc: (94.91%) (28883/30432)\n",
      "Epoch: 6 | Batch_idx: 1000 |  Loss: (0.1548) | Acc: (94.87%) (30388/32032)\n",
      "Epoch: 6 | Batch_idx: 1050 |  Loss: (0.1541) | Acc: (94.87%) (31908/33632)\n",
      "Epoch: 6 | Batch_idx: 1100 |  Loss: (0.1537) | Acc: (94.90%) (33434/35232)\n",
      "Epoch: 6 | Batch_idx: 1150 |  Loss: (0.1552) | Acc: (94.82%) (34925/36832)\n",
      "Epoch: 6 | Batch_idx: 1200 |  Loss: (0.1555) | Acc: (94.80%) (36433/38432)\n",
      "Epoch: 6 | Batch_idx: 1250 |  Loss: (0.1556) | Acc: (94.78%) (37942/40032)\n",
      "Epoch: 6 | Batch_idx: 1300 |  Loss: (0.1570) | Acc: (94.72%) (39432/41632)\n",
      "Epoch: 6 | Batch_idx: 1350 |  Loss: (0.1569) | Acc: (94.69%) (40938/43232)\n",
      "Epoch: 6 | Batch_idx: 1400 |  Loss: (0.1563) | Acc: (94.74%) (42474/44832)\n",
      "Epoch: 6 | Batch_idx: 1450 |  Loss: (0.1566) | Acc: (94.71%) (43975/46432)\n",
      "Epoch: 6 | Batch_idx: 1500 |  Loss: (0.1582) | Acc: (94.66%) (45465/48032)\n",
      "Epoch: 6 | Batch_idx: 1550 |  Loss: (0.1596) | Acc: (94.61%) (46959/49632)\n",
      "Epoch: 6 | Batch_idx: 1562 |  Loss: (0.1603) | Acc: (94.60%) (47298/50000)\n",
      "# TEST : Loss: (0.1902) | Acc: (93.69%) (9369/10000)\n",
      "8.18711994874345e-05\n",
      "\n",
      "Epoch: 7\n",
      "Epoch: 7 | Batch_idx: 0 |  Loss: (0.1042) | Acc: (96.88%) (31/32)\n",
      "Epoch: 7 | Batch_idx: 50 |  Loss: (0.1632) | Acc: (94.00%) (1534/1632)\n",
      "Epoch: 7 | Batch_idx: 100 |  Loss: (0.1566) | Acc: (94.43%) (3052/3232)\n",
      "Epoch: 7 | Batch_idx: 150 |  Loss: (0.1437) | Acc: (94.89%) (4585/4832)\n",
      "Epoch: 7 | Batch_idx: 200 |  Loss: (0.1399) | Acc: (95.06%) (6114/6432)\n",
      "Epoch: 7 | Batch_idx: 250 |  Loss: (0.1351) | Acc: (95.23%) (7649/8032)\n",
      "Epoch: 7 | Batch_idx: 300 |  Loss: (0.1340) | Acc: (95.29%) (9178/9632)\n",
      "Epoch: 7 | Batch_idx: 350 |  Loss: (0.1352) | Acc: (95.26%) (10700/11232)\n",
      "Epoch: 7 | Batch_idx: 400 |  Loss: (0.1370) | Acc: (95.26%) (12224/12832)\n",
      "Epoch: 7 | Batch_idx: 450 |  Loss: (0.1356) | Acc: (95.32%) (13756/14432)\n",
      "Epoch: 7 | Batch_idx: 500 |  Loss: (0.1334) | Acc: (95.40%) (15295/16032)\n",
      "Epoch: 7 | Batch_idx: 550 |  Loss: (0.1349) | Acc: (95.40%) (16821/17632)\n",
      "Epoch: 7 | Batch_idx: 600 |  Loss: (0.1357) | Acc: (95.35%) (18338/19232)\n",
      "Epoch: 7 | Batch_idx: 650 |  Loss: (0.1360) | Acc: (95.31%) (19854/20832)\n",
      "Epoch: 7 | Batch_idx: 700 |  Loss: (0.1381) | Acc: (95.25%) (21366/22432)\n",
      "Epoch: 7 | Batch_idx: 750 |  Loss: (0.1387) | Acc: (95.22%) (22883/24032)\n",
      "Epoch: 7 | Batch_idx: 800 |  Loss: (0.1401) | Acc: (95.17%) (24395/25632)\n",
      "Epoch: 7 | Batch_idx: 850 |  Loss: (0.1407) | Acc: (95.15%) (25911/27232)\n",
      "Epoch: 7 | Batch_idx: 900 |  Loss: (0.1414) | Acc: (95.13%) (27428/28832)\n",
      "Epoch: 7 | Batch_idx: 950 |  Loss: (0.1420) | Acc: (95.15%) (28955/30432)\n",
      "Epoch: 7 | Batch_idx: 1000 |  Loss: (0.1429) | Acc: (95.14%) (30475/32032)\n",
      "Epoch: 7 | Batch_idx: 1050 |  Loss: (0.1440) | Acc: (95.13%) (31994/33632)\n",
      "Epoch: 7 | Batch_idx: 1100 |  Loss: (0.1437) | Acc: (95.14%) (33520/35232)\n",
      "Epoch: 7 | Batch_idx: 1150 |  Loss: (0.1452) | Acc: (95.09%) (35025/36832)\n",
      "Epoch: 7 | Batch_idx: 1200 |  Loss: (0.1448) | Acc: (95.10%) (36548/38432)\n",
      "Epoch: 7 | Batch_idx: 1250 |  Loss: (0.1442) | Acc: (95.13%) (38083/40032)\n",
      "Epoch: 7 | Batch_idx: 1300 |  Loss: (0.1433) | Acc: (95.16%) (39619/41632)\n",
      "Epoch: 7 | Batch_idx: 1350 |  Loss: (0.1448) | Acc: (95.10%) (41115/43232)\n",
      "Epoch: 7 | Batch_idx: 1400 |  Loss: (0.1461) | Acc: (95.03%) (42604/44832)\n",
      "Epoch: 7 | Batch_idx: 1450 |  Loss: (0.1472) | Acc: (94.98%) (44103/46432)\n",
      "Epoch: 7 | Batch_idx: 1500 |  Loss: (0.1483) | Acc: (94.94%) (45601/48032)\n",
      "Epoch: 7 | Batch_idx: 1550 |  Loss: (0.1480) | Acc: (94.94%) (47121/49632)\n",
      "Epoch: 7 | Batch_idx: 1562 |  Loss: (0.1481) | Acc: (94.94%) (47469/50000)\n",
      "# TEST : Loss: (0.1750) | Acc: (94.30%) (9430/10000)\n",
      "7.679133974894983e-05\n",
      "\n",
      "Epoch: 8\n",
      "Epoch: 8 | Batch_idx: 0 |  Loss: (0.0578) | Acc: (96.88%) (31/32)\n",
      "Epoch: 8 | Batch_idx: 50 |  Loss: (0.1100) | Acc: (95.77%) (1563/1632)\n",
      "Epoch: 8 | Batch_idx: 100 |  Loss: (0.1038) | Acc: (96.29%) (3112/3232)\n",
      "Epoch: 8 | Batch_idx: 150 |  Loss: (0.1055) | Acc: (96.25%) (4651/4832)\n",
      "Epoch: 8 | Batch_idx: 200 |  Loss: (0.1091) | Acc: (96.25%) (6191/6432)\n",
      "Epoch: 8 | Batch_idx: 250 |  Loss: (0.1120) | Acc: (96.07%) (7716/8032)\n",
      "Epoch: 8 | Batch_idx: 300 |  Loss: (0.1129) | Acc: (96.02%) (9249/9632)\n",
      "Epoch: 8 | Batch_idx: 350 |  Loss: (0.1141) | Acc: (96.06%) (10790/11232)\n",
      "Epoch: 8 | Batch_idx: 400 |  Loss: (0.1165) | Acc: (95.96%) (12314/12832)\n",
      "Epoch: 8 | Batch_idx: 450 |  Loss: (0.1185) | Acc: (95.94%) (13846/14432)\n",
      "Epoch: 8 | Batch_idx: 500 |  Loss: (0.1194) | Acc: (95.88%) (15372/16032)\n",
      "Epoch: 8 | Batch_idx: 550 |  Loss: (0.1201) | Acc: (95.87%) (16904/17632)\n",
      "Epoch: 8 | Batch_idx: 600 |  Loss: (0.1230) | Acc: (95.77%) (18419/19232)\n",
      "Epoch: 8 | Batch_idx: 650 |  Loss: (0.1246) | Acc: (95.76%) (19949/20832)\n",
      "Epoch: 8 | Batch_idx: 700 |  Loss: (0.1259) | Acc: (95.73%) (21474/22432)\n",
      "Epoch: 8 | Batch_idx: 750 |  Loss: (0.1247) | Acc: (95.74%) (23008/24032)\n",
      "Epoch: 8 | Batch_idx: 800 |  Loss: (0.1256) | Acc: (95.73%) (24537/25632)\n",
      "Epoch: 8 | Batch_idx: 850 |  Loss: (0.1269) | Acc: (95.70%) (26062/27232)\n",
      "Epoch: 8 | Batch_idx: 900 |  Loss: (0.1278) | Acc: (95.65%) (27579/28832)\n",
      "Epoch: 8 | Batch_idx: 950 |  Loss: (0.1285) | Acc: (95.65%) (29107/30432)\n",
      "Epoch: 8 | Batch_idx: 1000 |  Loss: (0.1297) | Acc: (95.60%) (30621/32032)\n",
      "Epoch: 8 | Batch_idx: 1050 |  Loss: (0.1305) | Acc: (95.58%) (32147/33632)\n",
      "Epoch: 8 | Batch_idx: 1100 |  Loss: (0.1306) | Acc: (95.58%) (33676/35232)\n",
      "Epoch: 8 | Batch_idx: 1150 |  Loss: (0.1308) | Acc: (95.58%) (35203/36832)\n",
      "Epoch: 8 | Batch_idx: 1200 |  Loss: (0.1310) | Acc: (95.57%) (36729/38432)\n",
      "Epoch: 8 | Batch_idx: 1250 |  Loss: (0.1296) | Acc: (95.61%) (38276/40032)\n",
      "Epoch: 8 | Batch_idx: 1300 |  Loss: (0.1288) | Acc: (95.64%) (39816/41632)\n",
      "Epoch: 8 | Batch_idx: 1350 |  Loss: (0.1287) | Acc: (95.62%) (41339/43232)\n",
      "Epoch: 8 | Batch_idx: 1400 |  Loss: (0.1289) | Acc: (95.63%) (42872/44832)\n",
      "Epoch: 8 | Batch_idx: 1450 |  Loss: (0.1288) | Acc: (95.63%) (44402/46432)\n",
      "Epoch: 8 | Batch_idx: 1500 |  Loss: (0.1287) | Acc: (95.64%) (45936/48032)\n",
      "Epoch: 8 | Batch_idx: 1550 |  Loss: (0.1285) | Acc: (95.65%) (47471/49632)\n",
      "Epoch: 8 | Batch_idx: 1562 |  Loss: (0.1284) | Acc: (95.64%) (47820/50000)\n",
      "# TEST : Loss: (0.1843) | Acc: (94.15%) (9415/10000)\n",
      "7.128896457825363e-05\n",
      "\n",
      "Epoch: 9\n",
      "Epoch: 9 | Batch_idx: 0 |  Loss: (0.0967) | Acc: (96.88%) (31/32)\n",
      "Epoch: 9 | Batch_idx: 50 |  Loss: (0.0996) | Acc: (96.32%) (1572/1632)\n",
      "Epoch: 9 | Batch_idx: 100 |  Loss: (0.0915) | Acc: (97.15%) (3140/3232)\n",
      "Epoch: 9 | Batch_idx: 150 |  Loss: (0.0927) | Acc: (96.85%) (4680/4832)\n",
      "Epoch: 9 | Batch_idx: 200 |  Loss: (0.0925) | Acc: (96.81%) (6227/6432)\n",
      "Epoch: 9 | Batch_idx: 250 |  Loss: (0.0973) | Acc: (96.64%) (7762/8032)\n",
      "Epoch: 9 | Batch_idx: 300 |  Loss: (0.1014) | Acc: (96.52%) (9297/9632)\n",
      "Epoch: 9 | Batch_idx: 350 |  Loss: (0.1026) | Acc: (96.51%) (10840/11232)\n",
      "Epoch: 9 | Batch_idx: 400 |  Loss: (0.1047) | Acc: (96.43%) (12374/12832)\n",
      "Epoch: 9 | Batch_idx: 450 |  Loss: (0.1025) | Acc: (96.49%) (13925/14432)\n",
      "Epoch: 9 | Batch_idx: 500 |  Loss: (0.1043) | Acc: (96.48%) (15468/16032)\n",
      "Epoch: 9 | Batch_idx: 550 |  Loss: (0.1044) | Acc: (96.44%) (17004/17632)\n",
      "Epoch: 9 | Batch_idx: 600 |  Loss: (0.1063) | Acc: (96.38%) (18535/19232)\n",
      "Epoch: 9 | Batch_idx: 650 |  Loss: (0.1052) | Acc: (96.45%) (20093/20832)\n",
      "Epoch: 9 | Batch_idx: 700 |  Loss: (0.1056) | Acc: (96.45%) (21635/22432)\n",
      "Epoch: 9 | Batch_idx: 750 |  Loss: (0.1056) | Acc: (96.46%) (23182/24032)\n",
      "Epoch: 9 | Batch_idx: 800 |  Loss: (0.1060) | Acc: (96.44%) (24720/25632)\n",
      "Epoch: 9 | Batch_idx: 850 |  Loss: (0.1069) | Acc: (96.41%) (26255/27232)\n",
      "Epoch: 9 | Batch_idx: 900 |  Loss: (0.1072) | Acc: (96.38%) (27788/28832)\n",
      "Epoch: 9 | Batch_idx: 950 |  Loss: (0.1081) | Acc: (96.38%) (29331/30432)\n",
      "Epoch: 9 | Batch_idx: 1000 |  Loss: (0.1087) | Acc: (96.37%) (30868/32032)\n",
      "Epoch: 9 | Batch_idx: 1050 |  Loss: (0.1097) | Acc: (96.34%) (32402/33632)\n",
      "Epoch: 9 | Batch_idx: 1100 |  Loss: (0.1102) | Acc: (96.32%) (33935/35232)\n",
      "Epoch: 9 | Batch_idx: 1150 |  Loss: (0.1100) | Acc: (96.34%) (35484/36832)\n",
      "Epoch: 9 | Batch_idx: 1200 |  Loss: (0.1106) | Acc: (96.31%) (37014/38432)\n",
      "Epoch: 9 | Batch_idx: 1250 |  Loss: (0.1117) | Acc: (96.25%) (38531/40032)\n",
      "Epoch: 9 | Batch_idx: 1300 |  Loss: (0.1122) | Acc: (96.23%) (40064/41632)\n",
      "Epoch: 9 | Batch_idx: 1350 |  Loss: (0.1128) | Acc: (96.21%) (41592/43232)\n",
      "Epoch: 9 | Batch_idx: 1400 |  Loss: (0.1134) | Acc: (96.20%) (43128/44832)\n",
      "Epoch: 9 | Batch_idx: 1450 |  Loss: (0.1138) | Acc: (96.18%) (44660/46432)\n",
      "Epoch: 9 | Batch_idx: 1500 |  Loss: (0.1134) | Acc: (96.20%) (46206/48032)\n",
      "Epoch: 9 | Batch_idx: 1550 |  Loss: (0.1138) | Acc: (96.18%) (47735/49632)\n",
      "Epoch: 9 | Batch_idx: 1562 |  Loss: (0.1142) | Acc: (96.17%) (48084/50000)\n",
      "# TEST : Loss: (0.1961) | Acc: (93.67%) (9367/10000)\n",
      "6.545084971874737e-05\n",
      "\n",
      "Epoch: 10\n",
      "Epoch: 10 | Batch_idx: 0 |  Loss: (0.3403) | Acc: (87.50%) (28/32)\n",
      "Epoch: 10 | Batch_idx: 50 |  Loss: (0.1109) | Acc: (96.51%) (1575/1632)\n",
      "Epoch: 10 | Batch_idx: 100 |  Loss: (0.1089) | Acc: (96.47%) (3118/3232)\n",
      "Epoch: 10 | Batch_idx: 150 |  Loss: (0.1017) | Acc: (96.59%) (4667/4832)\n",
      "Epoch: 10 | Batch_idx: 200 |  Loss: (0.0986) | Acc: (96.67%) (6218/6432)\n",
      "Epoch: 10 | Batch_idx: 250 |  Loss: (0.0978) | Acc: (96.73%) (7769/8032)\n",
      "Epoch: 10 | Batch_idx: 300 |  Loss: (0.1010) | Acc: (96.65%) (9309/9632)\n",
      "Epoch: 10 | Batch_idx: 350 |  Loss: (0.1035) | Acc: (96.55%) (10845/11232)\n",
      "Epoch: 10 | Batch_idx: 400 |  Loss: (0.1021) | Acc: (96.63%) (12399/12832)\n",
      "Epoch: 10 | Batch_idx: 450 |  Loss: (0.1019) | Acc: (96.65%) (13949/14432)\n",
      "Epoch: 10 | Batch_idx: 500 |  Loss: (0.1014) | Acc: (96.67%) (15498/16032)\n",
      "Epoch: 10 | Batch_idx: 550 |  Loss: (0.1028) | Acc: (96.60%) (17032/17632)\n",
      "Epoch: 10 | Batch_idx: 600 |  Loss: (0.1015) | Acc: (96.59%) (18577/19232)\n",
      "Epoch: 10 | Batch_idx: 650 |  Loss: (0.1008) | Acc: (96.62%) (20127/20832)\n",
      "Epoch: 10 | Batch_idx: 700 |  Loss: (0.1004) | Acc: (96.67%) (21684/22432)\n",
      "Epoch: 10 | Batch_idx: 750 |  Loss: (0.0997) | Acc: (96.70%) (23239/24032)\n",
      "Epoch: 10 | Batch_idx: 800 |  Loss: (0.0990) | Acc: (96.72%) (24791/25632)\n",
      "Epoch: 10 | Batch_idx: 850 |  Loss: (0.0991) | Acc: (96.69%) (26330/27232)\n",
      "Epoch: 10 | Batch_idx: 900 |  Loss: (0.0994) | Acc: (96.69%) (27878/28832)\n",
      "Epoch: 10 | Batch_idx: 950 |  Loss: (0.0987) | Acc: (96.71%) (29431/30432)\n",
      "Epoch: 10 | Batch_idx: 1000 |  Loss: (0.0994) | Acc: (96.68%) (30969/32032)\n",
      "Epoch: 10 | Batch_idx: 1050 |  Loss: (0.1004) | Acc: (96.65%) (32504/33632)\n",
      "Epoch: 10 | Batch_idx: 1100 |  Loss: (0.1019) | Acc: (96.59%) (34029/35232)\n",
      "Epoch: 10 | Batch_idx: 1150 |  Loss: (0.1028) | Acc: (96.57%) (35567/36832)\n",
      "Epoch: 10 | Batch_idx: 1200 |  Loss: (0.1030) | Acc: (96.56%) (37110/38432)\n",
      "Epoch: 10 | Batch_idx: 1250 |  Loss: (0.1040) | Acc: (96.52%) (38640/40032)\n",
      "Epoch: 10 | Batch_idx: 1300 |  Loss: (0.1037) | Acc: (96.52%) (40185/41632)\n",
      "Epoch: 10 | Batch_idx: 1350 |  Loss: (0.1032) | Acc: (96.53%) (41733/43232)\n",
      "Epoch: 10 | Batch_idx: 1400 |  Loss: (0.1035) | Acc: (96.54%) (43281/44832)\n",
      "Epoch: 10 | Batch_idx: 1450 |  Loss: (0.1036) | Acc: (96.52%) (44815/46432)\n",
      "Epoch: 10 | Batch_idx: 1500 |  Loss: (0.1035) | Acc: (96.52%) (46360/48032)\n",
      "Epoch: 10 | Batch_idx: 1550 |  Loss: (0.1049) | Acc: (96.47%) (47880/49632)\n",
      "Epoch: 10 | Batch_idx: 1562 |  Loss: (0.1048) | Acc: (96.47%) (48235/50000)\n",
      "# TEST : Loss: (0.1546) | Acc: (95.05%) (9505/10000)\n",
      "5.936906572928624e-05\n",
      "\n",
      "Epoch: 11\n",
      "Epoch: 11 | Batch_idx: 0 |  Loss: (0.0150) | Acc: (100.00%) (32/32)\n",
      "Epoch: 11 | Batch_idx: 50 |  Loss: (0.0757) | Acc: (98.04%) (1600/1632)\n",
      "Epoch: 11 | Batch_idx: 100 |  Loss: (0.0772) | Acc: (97.87%) (3163/3232)\n",
      "Epoch: 11 | Batch_idx: 150 |  Loss: (0.0795) | Acc: (97.76%) (4724/4832)\n",
      "Epoch: 11 | Batch_idx: 200 |  Loss: (0.0786) | Acc: (97.73%) (6286/6432)\n",
      "Epoch: 11 | Batch_idx: 250 |  Loss: (0.0810) | Acc: (97.61%) (7840/8032)\n",
      "Epoch: 11 | Batch_idx: 300 |  Loss: (0.0809) | Acc: (97.59%) (9400/9632)\n",
      "Epoch: 11 | Batch_idx: 350 |  Loss: (0.0788) | Acc: (97.68%) (10971/11232)\n",
      "Epoch: 11 | Batch_idx: 400 |  Loss: (0.0777) | Acc: (97.69%) (12536/12832)\n",
      "Epoch: 11 | Batch_idx: 450 |  Loss: (0.0800) | Acc: (97.53%) (14076/14432)\n",
      "Epoch: 11 | Batch_idx: 500 |  Loss: (0.0808) | Acc: (97.48%) (15628/16032)\n",
      "Epoch: 11 | Batch_idx: 550 |  Loss: (0.0815) | Acc: (97.46%) (17185/17632)\n",
      "Epoch: 11 | Batch_idx: 600 |  Loss: (0.0818) | Acc: (97.49%) (18749/19232)\n",
      "Epoch: 11 | Batch_idx: 650 |  Loss: (0.0818) | Acc: (97.45%) (20301/20832)\n",
      "Epoch: 11 | Batch_idx: 700 |  Loss: (0.0839) | Acc: (97.33%) (21834/22432)\n",
      "Epoch: 11 | Batch_idx: 750 |  Loss: (0.0831) | Acc: (97.38%) (23402/24032)\n",
      "Epoch: 11 | Batch_idx: 800 |  Loss: (0.0828) | Acc: (97.39%) (24962/25632)\n",
      "Epoch: 11 | Batch_idx: 850 |  Loss: (0.0834) | Acc: (97.37%) (26516/27232)\n",
      "Epoch: 11 | Batch_idx: 900 |  Loss: (0.0845) | Acc: (97.35%) (28067/28832)\n",
      "Epoch: 11 | Batch_idx: 950 |  Loss: (0.0841) | Acc: (97.37%) (29632/30432)\n",
      "Epoch: 11 | Batch_idx: 1000 |  Loss: (0.0836) | Acc: (97.37%) (31191/32032)\n",
      "Epoch: 11 | Batch_idx: 1050 |  Loss: (0.0830) | Acc: (97.40%) (32756/33632)\n",
      "Epoch: 11 | Batch_idx: 1100 |  Loss: (0.0838) | Acc: (97.37%) (34304/35232)\n",
      "Epoch: 11 | Batch_idx: 1150 |  Loss: (0.0843) | Acc: (97.36%) (35861/36832)\n",
      "Epoch: 11 | Batch_idx: 1200 |  Loss: (0.0842) | Acc: (97.36%) (37417/38432)\n",
      "Epoch: 11 | Batch_idx: 1250 |  Loss: (0.0849) | Acc: (97.34%) (38966/40032)\n",
      "Epoch: 11 | Batch_idx: 1300 |  Loss: (0.0853) | Acc: (97.32%) (40517/41632)\n",
      "Epoch: 11 | Batch_idx: 1350 |  Loss: (0.0857) | Acc: (97.31%) (42067/43232)\n",
      "Epoch: 11 | Batch_idx: 1400 |  Loss: (0.0861) | Acc: (97.28%) (43613/44832)\n",
      "Epoch: 11 | Batch_idx: 1450 |  Loss: (0.0860) | Acc: (97.28%) (45169/46432)\n",
      "Epoch: 11 | Batch_idx: 1500 |  Loss: (0.0864) | Acc: (97.26%) (46717/48032)\n",
      "Epoch: 11 | Batch_idx: 1550 |  Loss: (0.0864) | Acc: (97.25%) (48269/49632)\n",
      "Epoch: 11 | Batch_idx: 1562 |  Loss: (0.0863) | Acc: (97.26%) (48631/50000)\n",
      "# TEST : Loss: (0.1594) | Acc: (94.77%) (9477/10000)\n",
      "5.313952597646568e-05\n",
      "\n",
      "Epoch: 12\n",
      "Epoch: 12 | Batch_idx: 0 |  Loss: (0.0230) | Acc: (100.00%) (32/32)\n",
      "Epoch: 12 | Batch_idx: 50 |  Loss: (0.0637) | Acc: (97.73%) (1595/1632)\n",
      "Epoch: 12 | Batch_idx: 100 |  Loss: (0.0609) | Acc: (98.08%) (3170/3232)\n",
      "Epoch: 12 | Batch_idx: 150 |  Loss: (0.0624) | Acc: (98.01%) (4736/4832)\n",
      "Epoch: 12 | Batch_idx: 200 |  Loss: (0.0629) | Acc: (97.99%) (6303/6432)\n",
      "Epoch: 12 | Batch_idx: 250 |  Loss: (0.0660) | Acc: (97.86%) (7860/8032)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    for epoch in range(start_epoch, start_epoch+epoch_num): \n",
    "        train_error.append(train(epoch))    \n",
    "        test_error.append(test(epoch))          \n",
    "        scheduler.step()\n",
    "        print(optimizer.param_groups[0]['lr'])\n",
    "        lerningrate_temp.append(optimizer.param_groups[0]['lr'])\n",
    "        \n",
    "        #print(train_error)\n",
    "        #print(test_error)\n",
    "    \n",
    "    plt.plot(train_error,label='train_acc')\n",
    "    plt.plot(test_error,label='test_acc')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    plt.plot(lerningrate_temp,label='lr')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
